philly
mean(fin.sig)
##Fixed hyperparameters
gt.sig=.0014
lt.sig=.0016
tausq.b=.05^2
b.gt.hyper=.5625
b.lt.hyper=.445
b1.hyper=.00056
b2.hyper=-.000639
tau.sig=10
trunc.sd=.01
n=30
##Initialization
numsamp=25000
beta.mat=matrix(0,nrow=numsamp,ncol=4)
sigsq.vec=numeric(numsamp)
beta.mat[1,]=c(.5,.5,.01,-.01)
sigsq.vec[1]=.01
beta.mat2=matrix(0,nrow=numsamp,ncol=4)
sigsq.vec2=numeric(numsamp)
beta.mat2[1,]=c(.3,.7,0,-.0)
sigsq.vec2[1]=.05
##Set-up for beta component
x.mat.temp=cbind(gt500,lt500,x.mat)
x.mat.temp
x.reg.mat=rbind(x.mat.temp,diag(4))
diag.mat=diag(n+4)
##Density
sig.dens=function(sse,sigsq,tau.sig){
lik=-30*.5*log(2*pi*sigsq)-sse/(2*sigsq)-.5*log(pi*tau.sig/2)-sigsq^2/(2*tau.sig)
return(lik)
}
##Gibbs Sampler
mh=function(numsamp,beta.mat,sigsq.vec){
chain=list()
for( i in 2:numsamp){
##beta step
cov.mat=sigsq.vec[i-1]*diag.mat
cov.mat[31,31]=gt.sig;cov.mat[32,32]=lt.sig;cov.mat[33,33]=tausq.b;cov.mat[34,34]=tausq.b;
y.reg=c(y.vec,b.gt.hyper,b.lt.hyper,b1.hyper,b2.hyper)
inv.mat=solve(cov.mat)
beta.hat=solve((t(x.reg.mat)%*%inv.mat%*%x.reg.mat))%*%t(x.reg.mat)%*%inv.mat%*%y.reg
beta.cov=solve(t(x.reg.mat)%*%inv.mat%*%(x.reg.mat))
beta.mat[i,]=mvrnorm(n=1,mu=beta.hat,Sigma=beta.cov)
##sigma step
df=2
ss.temp=(y.reg-x.reg.mat%*%beta.hat)
sse=sum(ss.temp^2)
##MH
temp=rtnorm(n=1,mean=sigsq.vec[i-1],sd=trunc.sd,lower=0)
pstar=sig.dens(sse,temp,tau.sig)
pold=sig.dens(sse,sigsq.vec[i-1],tau.sig)
gold=dtnorm(x=sigsq.vec[i-1],mean=temp,sd=trunc.sd,lower=0)
gstar=dtnorm(x=temp,mean=sigsq.vec[i-1],sd=trunc.sd,lower=0)
logr=pstar-pold+log(gold)-log(gstar)
uni=runif(1,0,1)
sigsq.vec[i]=ifelse(log(uni)<=logr,temp,sigsq.vec[i-1])
if(i%%500==0) print(i)
}
chain[[1]]=beta.mat
chain[[2]]=sigsq.vec
return(chain)
}
mh.chain1=mh(numsamp,beta.mat,sigsq.vec)
mh.chain2=mh(numsamp,beta.mat2,sigsq.vec2)
beta1=mh.chain1[[1]];beta2=mh.chain2[[1]]
sig1=mh.chain1[[2]];sig2=mh.chain2[[2]]
##Burn
burn.size=1000
post.burn.beta1=beta1[(burn.size+1):numsamp,]
post.burn.beta2=beta2[(burn.size+1):numsamp,]
post.burn.alpha1=alpha1[(burn.size+1):numsamp,]
post.burn.alpha2=alpha2[(burn.size+1):numsamp,]
post.burn.sig1=sig1[(burn.size+1):numsamp]
post.burn.sig2=sig2[(burn.size+1):numsamp]
##Aggregate
temp.beta=rbind(post.burn.beta1,post.burn.beta2)
temp.sig=c(post.burn.sig1,post.burn.sig2)
##Thinning
postburn.n=length(temp.sig)
##Thinning-every 200th sample
temp=30*c(1:(postburn.n/30))
fin.beta=temp.beta[temp,]
fin.sig=temp.sig[temp]
#Autocorr
par(mfrow=c(2,2))
par(mar=c(3,3,3,2), mgp=c(1.8,.75,0))
acf(fin.beta[,2],main="Autocorr. for Beta 2")
acf(fin.beta[,3],main="Autocorr. for Beta 3")
acf(fin.beta[,4],main="Autocorr. for Beta 4")
acf(fin.sig,main="Autocorr. for Sigma^2")
##Convegence Plots
par(mfrow=c(2,2))
minsig=min(sig1,sig2)
maxsig=max(sig1,sig2)
plot(1:numsamp,sig1,col=1,ylim=c(minsig,maxsig),type="l",main="Sigma^2",xlab="Iteration",ylab="Sigma^2")
lines(1:numsamp,sig2,col=2)
for(i in 2:4){
betamin=min(beta1[,i],beta2[,i])
betamax=max(beta1[,i],beta2[,i])
plot(1:numsamp,beta1[,i],col=1,ylim=c(betamin,betamax),type="l",main=paste("Beta",toString(i)),xlab="Iteration",ylab="Beta")
lines(1:numsamp,beta2[,i],col=2)
}
##Forecasting:
##Forecasting:
pred.mat=matrix(0,nrow=length(fin.sig),ncol=n)
x.pred=x.mat.temp
num.post=length(fin.sig)
for(i in 1:num.post){
pred.mat[i,]=apply(x.pred,1, function(x) sum(x*fin.beta[i,])+rnorm(1,0,sd=sqrt(fin.sig[i])))
}
colnames(pred.mat)=data[,1]
cur=matrix(t(rep(Pct12,each=num.post)),nrow=num.post,ncol=n)
agg.mat=(pred.mat+cur)/2
summary.mat=matrix(0,nrow=30,ncol=3)
rownames(summary.mat)=colnames(pred.mat)
colnames(summary.mat)=c("Posterior Mean", "2.5th Quantile","97.5th Quantile")
summary.mat[,1]=apply(agg.mat,2,mean)
summary.mat[,2]=apply(agg.mat,2,function(x) quantile(x,probs=.025))
summary.mat[,3]=apply(agg.mat,2,function(x) quantile(x,probs=.975))
round(summary.mat,3)
##Do the Phillies win?
NL.east=agg.mat[,15:19] #Grab NL east
winners=apply(NL.east,1,function (x) which(x==max(x)))
philly=sum(winners==5)/num.post
philly
##Fixed hyperparameters
gt.sig=.0014
lt.sig=.0016
tausq.b=.05^2
b.gt.hyper=.5625
b.lt.hyper=.445
b1.hyper=.00056
b2.hyper=-.000639
tau.sig=10
trunc.sd=.01
n=30
##Initialization
numsamp=25000
beta.mat=matrix(0,nrow=numsamp,ncol=4)
sigsq.vec=numeric(numsamp)
beta.mat[1,]=c(.5,.5,.01,-.01)
sigsq.vec[1]=.01
beta.mat2=matrix(0,nrow=numsamp,ncol=4)
sigsq.vec2=numeric(numsamp)
beta.mat2[1,]=c(.3,.7,0,-.0)
sigsq.vec2[1]=.05
##Set-up for beta component
x.mat.temp=cbind(gt500,lt500,x.mat)
x.mat.temp
x.reg.mat=rbind(x.mat.temp,diag(4))
diag.mat=diag(n+4)
##Density
sig.dens=function(sse,sigsq,tau.sig){
lik=-30*.5*log(2*pi*sigsq)-sse/(2*sigsq)-.5*log(pi*tau.sig/2)-sigsq^2/(2*tau.sig)
return(lik)
}
##Gibbs Sampler
mh=function(numsamp,beta.mat,sigsq.vec){
chain=list()
for( i in 2:numsamp){
##beta step
cov.mat=sigsq.vec[i-1]*diag.mat
cov.mat[31,31]=gt.sig;cov.mat[32,32]=lt.sig;cov.mat[33,33]=tausq.b;cov.mat[34,34]=tausq.b;
y.reg=c(y.vec,b.gt.hyper,b.lt.hyper,b1.hyper,b2.hyper)
inv.mat=solve(cov.mat)
beta.hat=solve((t(x.reg.mat)%*%inv.mat%*%x.reg.mat))%*%t(x.reg.mat)%*%inv.mat%*%y.reg
beta.cov=solve(t(x.reg.mat)%*%inv.mat%*%(x.reg.mat))
beta.mat[i,]=mvrnorm(n=1,mu=beta.hat,Sigma=beta.cov)
##sigma step
df=2
ss.temp=(y.reg-x.reg.mat%*%beta.hat)
sse=sum(ss.temp^2)
##MH
temp=rtnorm(n=1,mean=sigsq.vec[i-1],sd=trunc.sd,lower=0)
pstar=sig.dens(sse,temp,tau.sig)
pold=sig.dens(sse,sigsq.vec[i-1],tau.sig)
gold=dtnorm(x=sigsq.vec[i-1],mean=temp,sd=trunc.sd,lower=0)
gstar=dtnorm(x=temp,mean=sigsq.vec[i-1],sd=trunc.sd,lower=0)
logr=pstar-pold+log(gold)-log(gstar)
uni=runif(1,0,1)
sigsq.vec[i]=ifelse(log(uni)<=logr,temp,sigsq.vec[i-1])
if(i%%500==0) print(i)
}
chain[[1]]=beta.mat
chain[[2]]=sigsq.vec
return(chain)
}
mh.chain1=mh(numsamp,beta.mat,sigsq.vec)
mh.chain2=mh(numsamp,beta.mat2,sigsq.vec2)
beta1=mh.chain1[[1]];beta2=mh.chain2[[1]]
sig1=mh.chain1[[2]];sig2=mh.chain2[[2]]
##Burn
burn.size=1000
post.burn.beta1=beta1[(burn.size+1):numsamp,]
post.burn.beta2=beta2[(burn.size+1):numsamp,]
post.burn.alpha1=alpha1[(burn.size+1):numsamp,]
post.burn.alpha2=alpha2[(burn.size+1):numsamp,]
post.burn.sig1=sig1[(burn.size+1):numsamp]
post.burn.sig2=sig2[(burn.size+1):numsamp]
##Aggregate
temp.beta=rbind(post.burn.beta1,post.burn.beta2)
temp.sig=c(post.burn.sig1,post.burn.sig2)
##Thinning
postburn.n=length(temp.sig)
##Thinning-every 200th sample
temp=30*c(1:(postburn.n/30))
fin.beta=temp.beta[temp,]
fin.sig=temp.sig[temp]
#Autocorr
par(mfrow=c(2,2))
par(mar=c(3,3,3,2), mgp=c(1.8,.75,0))
acf(fin.beta[,2],main="Autocorr. for Beta 2")
acf(fin.beta[,3],main="Autocorr. for Beta 3")
acf(fin.beta[,4],main="Autocorr. for Beta 4")
acf(fin.sig,main="Autocorr. for Sigma^2")
##Convegence Plots
par(mfrow=c(2,2))
minsig=min(sig1,sig2)
maxsig=max(sig1,sig2)
plot(1:numsamp,sig1,col=1,ylim=c(minsig,maxsig),type="l",main="Sigma^2",xlab="Iteration",ylab="Sigma^2")
lines(1:numsamp,sig2,col=2)
for(i in 2:4){
betamin=min(beta1[,i],beta2[,i])
betamax=max(beta1[,i],beta2[,i])
plot(1:numsamp,beta1[,i],col=1,ylim=c(betamin,betamax),type="l",main=paste("Beta",toString(i)),xlab="Iteration",ylab="Beta")
lines(1:numsamp,beta2[,i],col=2)
}
##Forecasting:
##Forecasting:
pred.mat=matrix(0,nrow=length(fin.sig),ncol=n)
x.pred=x.mat.temp
num.post=length(fin.sig)
for(i in 1:num.post){
pred.mat[i,]=apply(x.pred,1, function(x) sum(x*fin.beta[i,])+rnorm(1,0,sd=sqrt(fin.sig[i])))
}
colnames(pred.mat)=data[,1]
cur=matrix(t(rep(Pct12,each=num.post)),nrow=num.post,ncol=n)
agg.mat=(pred.mat+cur)/2
summary.mat=matrix(0,nrow=30,ncol=3)
rownames(summary.mat)=colnames(pred.mat)
colnames(summary.mat)=c("Posterior Mean", "2.5th Quantile","97.5th Quantile")
summary.mat[,1]=apply(agg.mat,2,mean)
summary.mat[,2]=apply(agg.mat,2,function(x) quantile(x,probs=.025))
summary.mat[,3]=apply(agg.mat,2,function(x) quantile(x,probs=.975))
round(summary.mat,3)
##Do the Phillies win?
NL.east=agg.mat[,15:19] #Grab NL east
winners=apply(NL.east,1,function (x) which(x==max(x)))
philly=sum(winners==5)/num.post
philly
mean(fin.sig)
par(mfrow=c(2,2))
minsig=min(sig1,sig2)
maxsig=max(sig1,sig2)
plot(1:numsamp,sig1,col=1,ylim=c(minsig,maxsig),type="l",main="Sigma^2",xlab="Iteration",ylab="Sigma^2")
lines(1:numsamp,sig2,col=2)
for(i in 2:4){
betamin=min(beta1[,i],beta2[,i])
betamax=max(beta1[,i],beta2[,i])
plot(1:numsamp,beta1[,i],col=1,ylim=c(betamin,betamax),type="l",main=paste("Beta",toString(i)),xlab="Iteration",ylab="Beta")
lines(1:numsamp,beta2[,i],col=2)
}
mean(fin.sig)
NL.east=pred.mat[,15:19] #Grab NL east
winners=apply(NL.east,1,function (x) which(x==max(x)))
philly=sum(winners==5)/num.post
philly
boston <- read.table("http://www-stat.wharton.upenn.edu/~buja/STAT-541/boston.dat",sep="", header=T)
boston.lm=lm(MEDV~.,data=boston)
boston.lm=lm(MEDV~.,data=boston)
lm.boot <- function(lm.model, Nboot=10000) {
slope.mat <- summary(lm.model)$coefficients
X <- model.matrix(lm.model)
y <- fitted(lm.model) + resid(lm.model)
p <- ncol(X)
N <- nrow(X)
slopes.bs <- matrix(NA, nrow=Nboot, ncol=p)
for(iboot in 1:Nboot) {
sel <- sample(N, replace=T)
## slopes.bs[iboot,] <- coef(lm(y[sel] ~ X[sel,-1]))
slopes.bs[iboot,] <- solve(crossprod(X[sel,])) %*% t(X[sel,]) %*% y[sel]
if(iboot%%100==0) cat(iboot," ")
};  cat("\n")
SE.boot   <- apply(slopes.bs, 2, sd)
SE.ratio  <- SE.boot / slope.mat[,"Std. Error"]
t.boot    <- slope.mat[,"Estimate"] / SE.boot
slope.mat <- cbind(slope.mat, SE.boot=SE.boot, SE.ratio=SE.ratio, t.boot=t.boot)
slope.mat
}
boston.boot <- lm.boot(boston.lm)
summary(boston.lm)
signif(boston.boot, 3)
signif(boston.boot, 3)
signif(boston.boot, 3)
install.packages("acepack_1.3-2.2.zip", repos=NULL,
lib.loc="http://stat.wharton.upenn.edu/~buja/STAT-541/")
install.packages("~/Fall 11/STAT 541/acepack_1.3-2.2.zip", repos = NULL)
source('~/.active-rstudio-document', echo=TRUE)
library(acepack)
install.packages("acepack")
library(acepack)
boston.ace <- ace(x=boston[,-14], y=boston[,14])
boston.ace
names(boston.ace)
windows(width=3*3, height=5*3)
par(mfrow=c(5,3), mgp=c(1.5,0.2,0), mar=c(2.5,2.5,.5,.5), tck=-.02)
ylim <- range(boston.ace$tx)                 # shared range of the fi(Xi)
pch <- 18;  cex <- .7;  cex.lab <- .7;  cex.axis <- .7  # plot g(Y)
plot(boston.ace$y, boston.ace$ty, xlab="MEDV", ylab="g(MEDV)",
pch=pch, cex=cex, cex.lab=cex.lab, cex.axis=cex.axis)
for(i in 1:13)
plot(boston.ace$x[i,], boston.ace$tx[,i],  # plot fi(Xi)
ylim=ylim, xlab=dimnames(boston.ace$tx)[[2]][i], ylab="",
pch=pch, cex=cex, cex.lab=cex.lab, cex.axis=cex.axis)
signif(boston.boot, 3)
signif(boston.boot, 3)
windows(width=3*3, height=5*3)
par(mfrow=c(5,3), mgp=c(1.5,0.2,0), mar=c(2.5,2.5,.5,.5), tck=-.02)
ylim <- range(boston.ace$tx)                 # shared range of the fi(Xi)
pch <- 18;  cex <- .7;  cex.lab <- .7;  cex.axis <- .7  # plot g(Y)
plot(boston.ace$y, boston.ace$ty, xlab="MEDV", ylab="g(MEDV)",
pch=pch, cex=cex, cex.lab=cex.lab, cex.axis=cex.axis)
for(i in 1:13)
plot(boston.ace$x[i,], boston.ace$tx[,i],  # plot fi(Xi)
ylim=ylim, xlab=dimnames(boston.ace$tx)[[2]][i], ylab="",
pch=pch, cex=cex, cex.lab=cex.lab, cex.axis=cex.axis)
setwd("~/Criminology_Research/BART") ##work
load("working2.rdata")
source("aux_functions2.R")
temp=apply(work2,2,is.na)
apply(temp,2,sum) ##only AFirstChargeAge has NAs
##remove Na rows
work3=work2[-which(is.na(work2$AFirstChargeAge)),]
work4=work3[,-c(1,2,4,30)] ##get rid of some columns
xmat=work4[1:3000,]
response=work4[1:3000,1]
resp2=ifelse(response=="fail",5,0)
rf=randomForest(xmat[,-1],y=resp2,mtree=500)
library(randomForest)
rf=randomForest(xmat[,-1],y=resp2,mtree=500)
class(rf)
xmat=work4[3001:6000,]
predmat=work4[3001:6000,]
xmat=work4[1:3000,]
preds=predict(rf,newdata=predmat)
head(preds)
mean(response)
mean(response=="fail")
classes=ifelse(preds>.10133*5,1,0)
classes=ifelse(preds>.10133*5,"fail","No Fail")
conf_table(response,classes)
classes=ifelse(preds>1,"fail","No Fail")
conf_table(response,classes)
classes=ifelse(preds>5/6,"fail","No Fail")
conf_table(response,classes)
rf=randomForest(xmat[,-1],y=response,mtree=500,sampsize=c(200,200))
conf_table(response,rf2$preds)
rf2=randomForest(xmat[,-1],y=response,mtree=500,sampsize=c(200,200))
conf_table(response,rf2$preds)
xmat=work4[1:3000,]
rf2=randomForest(xmat[,-1],y=response,mtree=500,sampsize=c(200,200))
conf_table(response,rf2$preds)
length(response)
length(rf2$preds)
conf_table(response,rf2$pred)
conf_table(response,predict(rf2,predmat,type="class"))
conf_table(predmat[,1],classes)
train.preds=rf$pred
classes.train=ifelse(train.preds>5/6,"fail","No Fail")
train.preds
rf=randomForest(xmat[,-1],y=resp2,mtree=500)
train.preds=rf$pred
classes.train=ifelse(train.preds>5/6,"fail","No Fail")
conf_table(response,classes.train)
preds=predict(rf,newdata=predmat)
classes.test=ifelse(preds>5/6,"fail","No Fail")
conf_table(predmat[,1],classes.test)
plotRB(clean4,"rfTrain","rfTest","impliedCost",pch=16,xlab="RF Train Data",ylab="RF Test Data"
,main="Implied Cost")
hashNames=c("bartTrain","bartTest","rfTrain","rfTest")
getResultMatrix=function(calcList,hashNames){
temp=list()
output=list()
for(j in 1:4){
mat=matrix(nrow=length(calcList),ncol=6)
for(i in 1:length(calcList)){
mat[i,1]=calcList[[i]][[j]][[1]][[2]][[1]] ##modelErrorFail
mat[i,2]=calcList[[i]][[j]][[1]][[2]][[2]]  ##model error NoFail
mat[i,3]=calcList[[i]][[j]][[2]][[2]][[1]] ##UseErrorFail
mat[i,4]=calcList[[i]][[j]][[2]][[2]][[2]]  ##Use error NoFail
mat[i,5]=calcList[[i]][[j]][[3]][[2]] #overall error
mat[i,6]=calcList[[i]][[j]][[4]][[2]] ##implied cost
}
colnames(mat)=c("modErrFail","modErrNoFail","useErrFail","useErrNoFail","overallErr","impliedCost")
temp[[hashNames[j]]]=mat
##clean out for RFs that didn't work
}
exc=which(temp[["rfTest"]][,"impliedCost"]==Inf)
if(length(exc!=0)){
for(i in 1:4){
output[[hashNames[i]]]=temp[[i]][-exc,]
}
}
else {output=temp}
return(output)
}
plotRB=function(data,xName,yName,metric,...){
xdata=data[[xName]][,metric]
ydata=data[[yName]][,metric]
lower=min(min(xdata),min(ydata))
upper=max(max(xdata),max(ydata))
plot(xdata,ydata,xlim=c(lower,upper),ylim=c(lower,upper),...)
abline(0,1,col="red")
}
plotRB(clean4,"rfTrain","rfTest","impliedCost",pch=16,xlab="RF Train Data",ylab="RF Test Data"
"rfTrain","rfTest","impliedCost",pch=16,xlab="RF Train Data",ylab="RF Test Data"
plotRB(clean4,"rfTrain","rfTest","impliedCost",pch=16,xlab="RF Train Data",ylab="RF Test Data"
,main="Implied Cost")
load("bartrf50.rdata")
plotRB(clean4,"rfTrain","rfTest","impliedCost",pch=16,xlab="RF Train Data",ylab="RF Test Data"
,main="Implied Cost")
plotRB(clean5,"bartTrain","bartTest","impliedCost",pch=16,xlab="BART Train Data",ylab="BART Test Data"
,main="Implied Cost")
plotRB(clean4,"bartTrain","bartTest","impliedCost",pch=16,xlab="BART Train Data",ylab="BART Test Data"
,main="Implied Cost")
save(item50_1,file="item50_1.rdata")
save(list2000High,file="list2000High.rdata")
ls
ls
ls()
clean5=getResultMatrix(list2000High,hashNames)
setwd("~/Criminology_Research/BART") ##work
load("working2.rdata")
source("aux_functions2.R")
source("bart_crim_fns.R")
setwd("~/Criminology_Research/BART") ##work
load("working2.rdata")
source("aux_functions2.R")
setwd("~/Criminology_Research/BART/bart_crim") ##work
load("working2.rdata")
load("~/Criminology_Research/BARTworking2.rdata")
load("~/Criminology_Research/BART/working2.rdata")
source("aux_functions2.R")
source("bart_crim_fns.R")
work3=work2[-which(is.na(work2$AFirstChargeAge)),]
work4=work3[,-c(1,2,4,30)] ##get rid of some columns
work=work4;i=1
train_size=1000
test_size=900
p.fail=.48;p.nofail=1-p.fail
num.rows=sample(x=1:nrow(work),(train_size+test_size),replace=F) ##draw rows
train.rows=num.rows[1:train_size]
test.rows=num.rows[(train_size+1):length(num.rows)] #get disjoint set
samp=work[train.rows,] #sample data
samp[1,]=work[1,] ##include him for stratify to not get screwed up.
test=work[test.rows,] #test data
print(dim(test))
train.rows=num.rows[1:train_size]
test.rows=num.rows[(train_size+1):length(num.rows)] #get disjoint set
#Set test/train split-disjoint
samp=work[train.rows,] #sample data
samp[1,]=work[1,] ##include him for stratify to not get screwed up.
test=work[test.rows,] #test data
print(dim(test))
alt.rows=alter_prior(samp,p.nofail,p.fail,"FailSerious") #alter prior
train.data=samp[alt.rows,] ##training data
response=train.data[,1] ##set response in training set with altered prior
print(table(response))
alt.rows=alter_prior(samp,p.nofail,p.fail,"FailSerious") #alter prior
source("aux_functions2.R")
source("bart_crim_fns.R")
require(BayesTree)
require(randomForest)
require(sampling)
alt.rows=alter_prior(samp,p.nofail,p.fail,"FailSerious") #alter prior
train.data=samp[alt.rows,] ##training data
response=train.data[,1] ##set response in training set with altered prior
print(table(response))
alt.rows.test=alter_prior(samp,p.nofail,p.fail,"FailSerious") #alter prior
test.data.alt=samp[alt.rows.test,] ##training data
print(table(test.data.alt[,1]))
bart.mod=bart(x.train=train.data[,-1],y.train=response,x.test=samp[,-1])
alt.rows.test=alter_prior(test,p.nofail,p.fail,"FailSerious") #alter prior
test.data.alt=test[alt.rows.test,] ##training data
print(table(test.data.alt[,1]))
rownames(test.data.alt)
intersection(rownames(test),rownames(test.data.alt))
intersect(rownames(test),rownames(test.data.alt))
probs=pnorm(bart.mod$yhat.test)
mean.probs=apply(probs,2,mean)
class=ifelse(mean.probs>=.5,"No Fail","Fail")
print("Train Table")
trainTab=conf_table_stored_print(samp[,1],class)
impliedCost=as.numeric(trainTab[[4]][2])
storage[[i]][[1]]=trainTab
bart.test=bart(x.train=train.data[-1],y.train=response,x.test=test.data.alt[,-1],...) ##get predictions for original and build table
bart.test=bart(x.train=train.data[-1],y.train=response,x.test=test.data.alt[,-1]) ##
test.alt.probs=pnorm(bar.test$yhat.test)
test.alt.probs=pnorm(bart.test$yhat.test)
test.alt.probs=pnorm(bart.test$yhat.test)
mean.test.alt.probs=apply(test.alt.probs,2,mean)
test.class.alt=ifelse(mean.test.alt.probs>=.5,"No Fail","Fail")
rownames(test.class.alt)=rownames(test.alt.data)
rownames(test.class.alt)=rownames(test.data.alt)
test.class.alt=ifelse(mean.test.alt.probs>=.5,"No Fail","Fail")
rownames(test.class.alt)=rownames(test.data.alt)
names(test.class.alt)=rownames(test.data.alt)
common=intersect(rownames(test),rownames(test.data.alt))
conf_table(test[common,1],test.class.at[common])
conf_table(test[common,1],test.class.alt[common])
